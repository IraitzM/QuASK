{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d15079b-8f4a-4f61-9742-b4c0d5cd27d6",
   "metadata": {},
   "source": [
    "# Projected quantum kernels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b427a929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pennylane_noiseless': <function create_pennylane_noiseless at 0x732050fa2340>}\n",
      "pennylane_noiseless\n"
     ]
    }
   ],
   "source": [
    "# se the default implementation you want to use\n",
    "from quask.core import Ansatz, KernelFactory, KernelType\n",
    "from quask.core_implementation import PennylaneKernel\n",
    "\n",
    "def create_pennylane_noiseless(ansatz: Ansatz, measurement: str, type: KernelType):\n",
    "    return PennylaneKernel(ansatz, measurement, type, device_name=\"default.qubit\", n_shots=None)\n",
    "\n",
    "KernelFactory.add_implementation('pennylane_noiseless', create_pennylane_noiseless)\n",
    "KernelFactory.set_current_implementation('pennylane_noiseless')\n",
    "print(KernelFactory._KernelFactory__implementations)\n",
    "print(KernelFactory._KernelFactory__current_implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb156b-61e8-43db-8074-8c24e908f6f1",
   "metadata": {},
   "source": [
    "To understand Projected Quantum Kernels we should understand the limitations\n",
    "of \"traditional\" quantum kernels. These limitations have very deep implications\n",
    "in the understanding of kernel methods also on a classical perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc7a86-9592-4576-ae55-49fa9c68405e",
   "metadata": {},
   "source": [
    "## Expressibility and curse of dimensionality in kernel methods\n",
    "\n",
    "When approaching a ML problem we could ask if it makes sense at all to use QML techniques, such as quantum kernel methods. We understood in the last years \\[kbs21\\],\\[Hua21\\] that having a large Hilbert space where we can compute classically intractable inner products does not guarantee an advantage. But, why?\n",
    "\n",
    "When dealing with kernel methods, whether classical or quantum, we must exercise caution when working in high-dimensional (or even infinite-dimensional) Hilbert spaces. This is due to the fact that in\n",
    "high dimensions, the problem of generalization becomes hard, _i.e._ the trained kernel is prone to overfitting.\n",
    "In turn, an exponential (in the number of features/qubits) number of datapoints are needed to learn the target function we aim to estimate. These phenomena are explored in the [upcoming tutorial](xxx).\n",
    "\n",
    "For instance, in the classical context, the Gaussian kernel maps any $\\mathbf{x} \\in \\mathbb{R}^d$ to a multi-dimensional Gaussian distribution with an average of $\\mathbf{x}$ and a covariance matrix of $\\sigma I$. When $\\sigma$ is small, data points are mapped to different regions of this infinite-dimensional Hilbert space, and $\\kappa(\\mathbf{x}, \\mathbf{x}') \\approx 0$ for all $\\mathbf{x} \\neq \\mathbf{x}'$. This is known as the phenonenon of _curse of dimensionality_, or _orthogonality catastrophe_. To avoid this, a larger $\\sigma$ is chosen to ensure that most data points relevant to our task have some nontrivial overlap.\n",
    "\n",
    "As the Hilbert space for quantum systems grows exponentially with the number of qubits $n$, similar challenges can arise when using quantum kernels. This situation occurs with expressible $U(\\cdot)$, which allows access to various regions within the Hilbert space. In such cases, similar to classical kernels, techniques must be employed to control expressibility and, consequently, the model's complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d015e55-133a-4f59-aff6-c292aaee05d1",
   "metadata": {},
   "source": [
    "### Projection as expressibility control\n",
    "\n",
    "The authors of \\[Hua21\\], who initially addressed the challenge of the exponential dimensionality of Hilbert space in the context of quantum kernels, have introduced the concept of _projected quantum kernels_ to mitigate this issue. Then, in \\[kbs21\\] they proved as this projected kernel must intertwine with a correct inductive bias to obtain some good performance.\n",
    "\n",
    "The concept is straightforward: first, the unitary transformation $U$ maps classical data into the Hilbert space of the quantum system. Subsequently, a projection maps these elements back to a lower-dimensional Hilbert space. The overall transformation, thanks to the contribution of $U$, remains beyond the capabilities of classical kernels.\n",
    "\n",
    "For a single data point encoded in the quantum system, denoted as $\\rho_x = U(x) \\rho_0 U(x)$, projected quantum kernels can be implemented in two different ways:\n",
    "- We can implement the feature map $\\phi(x) = \\mathrm{\\tilde{Tr}}[\\rho_x]$, with $\\mathrm{\\tilde{Tr}}$ representing partial trace.\n",
    "- Alternatively, we can implement the feature map $\\phi(x) = \\{ \\mathrm{Tr}[\\rho_x O^{(j)}] \\}_{j=1}^k$, where the observable $O^{(j)}$ is employed for the projections.\n",
    "\n",
    "Finally, the kernel $\\kappa(x, x')$ is explicitly constructed as the inner product between $\\phi(x)$ and $\\phi(x')$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd1083-1bd1-4651-8113-1020fa96b1e8",
   "metadata": {},
   "source": [
    "### Implementation of projected quantum kernel in _quask_\n",
    "\n",
    "We first create the parameterized quantum circuit $U$ as in the previous tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d0e2df-e91a-48ed-8410-b83a580c1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quask.core import Ansatz, Kernel, KernelFactory, KernelType\n",
    "\n",
    "N_FEATURES = 2\n",
    "N_OPERATIONS = 3\n",
    "N_QUBITS = 2\n",
    "ansatz = Ansatz(n_features=N_FEATURES, n_qubits=N_QUBITS, n_operations=N_OPERATIONS)\n",
    "ansatz.initialize_to_identity()\n",
    "ansatz.change_operation(0, new_feature=0, new_wires=[0, 1], new_generator=\"ZZ\", new_bandwidth=1.0)\n",
    "ansatz.change_operation(1, new_feature=1, new_wires=[0, 1], new_generator=\"XX\", new_bandwidth=1.0)\n",
    "ansatz.change_operation(2, new_feature=2, new_wires=[0, 1], new_generator=\"IX\", new_bandwidth=0.123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8e11d-4858-46fe-9ea7-7198d905092d",
   "metadata": {},
   "source": [
    "Now, by employing the SWAP test over a subset of the $n$ qubits, only a small and constant number of qubits are measured, hence projecting in the subspace relative to those qubit selected by the SWAP test. This calculation is equivalent to performing the inner product between partial traces of two quantum-encoded data points. \n",
    "\n",
    "In the following example, the measurement is performed only on the first of two qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4533f360-52cf-482b-812b-bd1cd770f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = KernelFactory.create_kernel(ansatz, \"ZI\", KernelType.SWAP_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5eecf2-0c12-42f6-831b-f92e2cb4a786",
   "metadata": {},
   "source": [
    "We can also obtain the kernel by projecting onto a single observable described by a Pauli string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c04a61-b685-4fd2-8edb-59db83481a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = KernelFactory.create_kernel(ansatz, \"XY\", KernelType.OBSERVABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dfd18b-69f9-4d16-a77a-333537011e52",
   "metadata": {},
   "source": [
    "Multiple observable can be tested if we compose together kernel functions made of different observables. Due to the properties of positive semidefinite functions, the sum and product and tensor of positive semidefinite operators is again positive semidefinite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730358bc-6945-452a-bd63-dec11173be8f",
   "metadata": {},
   "source": [
    "## Learning of quantum processes\n",
    "\n",
    "The projected quantum kernel finds application in the realm of learning a quantum process, described by a function:\n",
    "\n",
    "$$f(x) = \\mathrm{Tr}[U^\\dagger(x) \\rho_0 U(x) O]$$\n",
    "\n",
    "Here, $U$ represents a parameterized quantum circuit, $\\rho_0$ is the initial state, and $O$ stands for the observable. This family of functions carries significant theoretical importance, as it has facilitated the formal demonstration of quantum advantages. It also holds practical significance, as certain use cases in physics and chemistry can be conceptualized as quantum processes.\n",
    "\n",
    "We are given a dataset, denoted as $\\{ (x^{(j)}, y^{(j)}) \\}_{j=1}^m$. Additionally, we assume that each label in this dataset is noise-free, meaning that $y^{(j)} = f(x^{(j)})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9caac0",
   "metadata": {},
   "source": [
    "## Create a Quantum Dataset \n",
    "\n",
    "We can use a $U$ quantum circuit inspired by quantum many-body physics, e.g. a Ising Hamiltonian with transverse field (open boundary condition is implemented by linear entanglement),\n",
    "$$ H = \\sum_{i=1}^{n-1} \\sigma_i^z \\sigma_{i+1}^z + \\lambda \\sum_{i=1}^n \\sigma_i^x $$\n",
    "where $n$ is the number of qubits.\n",
    "\n",
    "Then, U for the $j-th$ datapoint $\\{x_j^{(i)}\\}_{i=1}^n$ (which has $i$ number of features equal to the number of qubits $n$) will look like:\n",
    "$$ U (x_j) = \\exp(\\sum_{i=1}^{n-1}x_{ji}x_{ji+1}\\sigma_i^z \\sigma_{i+1}^z + \\sum_{i=1}^n x_{ji}\\sigma_i^x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d019c62-f19c-45ac-9c7b-835d2c3e7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us define the Ansatz which implements this\n",
    "def ManyBodyAnsatz(N_FEATURES, N_QUBITS, N_OPERATIONS, bandwidth):\n",
    "    ansatz = Ansatz(n_features=N_FEATURES, n_qubits=N_QUBITS, n_operations=N_OPERATIONS)\n",
    "    ansatz.initialize_to_identity()\n",
    "    # linear CZ entanglement\n",
    "    ansatz.change_operation(0, new_feature=4, new_wires=[0, 1], new_generator=\"ZZ\", new_bandwidth=bandwidth)\n",
    "    ansatz.change_operation(1, new_feature=5, new_wires=[1, 2], new_generator=\"ZZ\", new_bandwidth=bandwidth)\n",
    "    ansatz.change_operation(2, new_feature=6, new_wires=[2, 3], new_generator=\"ZZ\", new_bandwidth=bandwidth)\n",
    "    # single qubit rotations in x: RX wall\n",
    "    ansatz.change_operation(3, new_feature=0, new_wires=[0, 1], new_generator=\"XI\", new_bandwidth=bandwidth)\n",
    "    ansatz.change_operation(4, new_feature=1, new_wires=[1, 2], new_generator=\"XI\", new_bandwidth=bandwidth)\n",
    "    ansatz.change_operation(5, new_feature=2, new_wires=[2, 3], new_generator=\"XI\", new_bandwidth=bandwidth)\n",
    "    ansatz.change_operation(6, new_feature=3, new_wires=[3, 0], new_generator=\"XI\", new_bandwidth=bandwidth)\n",
    "    return ansatz\n",
    "\n",
    "# help funciton to select on which qubit we want to perform our measurement\n",
    "def measurement_qubit(N_QUBITS, measurement):\n",
    "    pauli_string = \"\"\n",
    "    for i in range(N_QUBITS):\n",
    "        if i < measurement:\n",
    "            pauli_string = pauli_string + \"I\"\n",
    "        if i == measurement:\n",
    "            pauli_string = pauli_string + \"Z\"\n",
    "        if i > measurement:\n",
    "            pauli_string = pauli_string + \"I\"\n",
    "    return pauli_string\n",
    "\n",
    "# we define the function which will give us quantum labels for our dataset\n",
    "def QuantumTargetFunction(ansatz, N_QUBITS, measurement):\n",
    "    kernel = KernelFactory.create_kernel(ansatz, measurement_qubit(N_QUBITS=N_QUBITS, measurement=measurement), KernelType.OBSERVABLE)\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4d384",
   "metadata": {},
   "source": [
    "Now, we can take a classical dataset, e.g. the _iris_ (check tutorial [Getting Started](\"../getting_started.html\")) dataset and change its labels with the quantum ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b053fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import linear_kernel, rbf_kernel\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "N_ELEMENTS_PER_CLASS = 10\n",
    "iris = load_iris()\n",
    "X = np.vstack([iris.data[0:N_ELEMENTS_PER_CLASS], iris.data[50:50+N_ELEMENTS_PER_CLASS]])\n",
    "y = np.array([0] * N_ELEMENTS_PER_CLASS + [1] * N_ELEMENTS_PER_CLASS)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f54614",
   "metadata": {},
   "source": [
    "As explained already in the first tutorial on [Quantum Kernels](../tutorials_quantum/quantum_0_intro.html), The features of the Unitary operator can be customed. We need to add them to a datapoint's feature. Here $x^{(j)}$ lives in a $\\mathbb{R}^4$ space. We can add, for the $i,i+1$ $ZZ$ interaction, the product of the nearest-neighbor features to the datapoint. Hence, $x = \\{x_0,x_1,x_2,x_3,x_0x_1,x_1x_2,x_2x_3\\} \\in \\mathbb{R}^7$. In this way, our `ManyBodyAnsatz` knows where to take the features to build the parametrized circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666e7285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 7)\n"
     ]
    }
   ],
   "source": [
    "X_new = []\n",
    "for x in X:\n",
    "    for i in range(x.shape[0]-1):\n",
    "        x = np.append(x, x[i]*x[i+1])\n",
    "    X_new.append(x)\n",
    "\n",
    "print(np.asarray(X_new).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ca0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the ansatz to sample from a quantum distribution\n",
    "N_FEATURES = 7\n",
    "N_QUBITS = 4\n",
    "N_OPERATIONS = 7\n",
    "bandwidth = 1\n",
    "ansatz = ManyBodyAnsatz(N_FEATURES=N_FEATURES, N_QUBITS=N_QUBITS, N_OPERATIONS=N_OPERATIONS, bandwidth=bandwidth)\n",
    "kernel_proj_1 = QuantumTargetFunction(ansatz=ansatz, N_QUBITS=N_QUBITS, measurement=0)\n",
    "kernel_proj_4 = QuantumTargetFunction(ansatz=ansatz, N_QUBITS=N_QUBITS, measurement=3)\n",
    "kernel_fid = KernelFactory.create_kernel(ansatz, \"Z\" * N_QUBITS, KernelType.FIDELITY)\n",
    "y_quantum = [kernel_proj_1.kappa(x,x) for x in X_new]\n",
    "new_labels = [1 if y >= 1/2 else 0 for y in y_quantum]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c21fc0",
   "metadata": {},
   "source": [
    "We have created a quantum dataset with labels, $i.e.$ classes, sampled from a quantum distribution given by our quantum feature map.\n",
    "Now, we can use the dataset as always to test our kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f1fdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy projection on (first) qubit 1: 0.7\n",
      "Accuracy projection on (last) qubit 4: 0.4\n",
      "Accuracy fidelity: 0.3\n",
      "Accuracy linear: 0.6\n",
      "Accuracy rbf: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_new = np.array(X_new)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, new_labels, test_size=0.5, random_state=5454)\n",
    "\n",
    "# Instantiate a machine learning model\n",
    "model_proj_1 = SVC(kernel='precomputed')\n",
    "model_proj_4 = SVC(kernel='precomputed')\n",
    "model_fid = SVC(kernel='precomputed')\n",
    "model_lin = SVC(kernel='precomputed')\n",
    "model_rbf = SVC(kernel='precomputed')\n",
    "\n",
    "# Fit the model to the training data\n",
    "K_train_1 = kernel_proj_1.build_kernel(X_train, X_train)\n",
    "model_proj_1.fit(K_train_1, y_train)\n",
    "K_train_4 = kernel_proj_4.build_kernel(X_train, X_train)\n",
    "model_proj_4.fit(K_train_4, y_train)\n",
    "K_train_fid = kernel_fid.build_kernel(X_train, X_train)\n",
    "model_fid.fit(K_train_fid, y_train)\n",
    "K_train_lin = linear_kernel(X_train)\n",
    "model_lin.fit(K_train_lin, y_train)\n",
    "K_train_rbf = rbf_kernel(X_train)\n",
    "model_rbf.fit(K_train_rbf, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "K_test_1 = kernel_proj_1.build_kernel(X_test, X_train)\n",
    "y_pred_1 = model_proj_1.predict(K_test_1)\n",
    "K_test_4 = kernel_proj_4.build_kernel(X_test, X_train)\n",
    "y_pred_4 = model_proj_4.predict(K_test_4)\n",
    "K_test_fid = kernel_fid.build_kernel(X_test, X_train)\n",
    "y_pred_fid = model_fid.predict(K_test_fid)\n",
    "K_test_lin = linear_kernel(X_test, X_train)\n",
    "y_pred_lin = model_lin.predict(K_test_lin)\n",
    "K_test_rbf = rbf_kernel(X_test, X_train)\n",
    "y_pred_rbf = model_rbf.predict(K_test_rbf)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_1 = np.sum(y_test == y_pred_1) / len(y_test)\n",
    "print(\"Accuracy projection on (first) qubit 1:\", accuracy_1)\n",
    "accuracy_4 = np.sum(y_test == y_pred_4) / len(y_test)\n",
    "print(\"Accuracy projection on (last) qubit 4:\", accuracy_4)\n",
    "accuracy_fid = np.sum(y_test == y_pred_fid) / len(y_test)\n",
    "print(\"Accuracy fidelity:\", accuracy_fid)\n",
    "accuracy_lin = np.sum(y_test == y_pred_lin) / len(y_test)\n",
    "print(\"Accuracy linear:\", accuracy_lin)\n",
    "accuracy_rbf = np.sum(y_test == y_pred_rbf) / len(y_test)\n",
    "print(\"Accuracy rbf:\", accuracy_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f41437f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(K_train_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9cfbe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.03671272559474694), np.float64(0.5715453830052948), np.float64(0.7968808921734044), np.float64(0.09647781596276594), np.float64(0.1805535420850637), np.float64(0.5715453830052948), np.float64(0.21989891011575627), np.float64(0.9993946120620129), np.float64(0.2838746697951175), np.float64(0.030764066789510918)]\n",
      "[0, 1, 1, 0, 0, 1, 0, 1, 0, 0]\n",
      "Training accuracy projection on (first) qubit 1: 0.8\n",
      "Training accuracy projection on (last) qubit 4: 0.6\n",
      "Training accuracy fidelity: 0.9\n",
      "Training accuracy linear: 0.9\n",
      "Training accuracy rbf: 0.9\n"
     ]
    }
   ],
   "source": [
    "print([K_train_1[i,i] for i in range(len(K_train_1))])\n",
    "print(y_train)\n",
    "\n",
    "accuracy_1 = np.sum(y_train == model_proj_1.predict(K_train_1)) / len(y_train)\n",
    "accuracy_4 = np.sum(y_train == model_proj_4.predict(K_train_4)) / len(y_train)\n",
    "accuracy_fid = np.sum(y_train == model_fid.predict(K_train_fid)) / len(y_train)\n",
    "accuracy_lin = np.sum(y_train == model_lin.predict(K_train_lin)) / len(y_train)\n",
    "accuracy_rbf = np.sum(y_train == model_rbf.predict(K_train_rbf)) / len(y_train)\n",
    "print(\"Training accuracy projection on (first) qubit 1:\", accuracy_1)\n",
    "print(\"Training accuracy projection on (last) qubit 4:\", accuracy_4)\n",
    "print(\"Training accuracy fidelity:\", accuracy_fid)\n",
    "print(\"Training accuracy linear:\", accuracy_lin)\n",
    "print(\"Training accuracy rbf:\", accuracy_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d747e21-af0d-4604-8ba2-c3aba23a4768",
   "metadata": {},
   "source": [
    "### S-value\n",
    "\n",
    "We can train a kernel machine on a dataset using a kernel $\\kappa$. The resulting model takes the form $h(x) = w^\\top \\phi(x)$. \n",
    "This representation is a kernel machine in its primal form, and the corresponding kernel Gram matrix is defined as $K = [\\kappa(x^{(i)}, x^{(j)})]_{i,j=1}^m$.  Assuming that the kernel Gram matrix is normalized, i.e., $\\mathrm{Tr}[K]=m$, we can define the _s-value_, a quantity that depends on the process $f$, the input data, and the kernel Gram matrix $K$:\n",
    "\n",
    "$$s_K = \\sum_{i,j=1}^m (K_{i,j}^{-1}) \\, f(x^{(i)}) \\, f(x^{(j)})$$\n",
    "\n",
    "This value quantifies how well the kernel function captures the behavior of the quantum process. The kernel is indeed able to capture the relationship within the data if:\n",
    "\n",
    "$$\\kappa(x^{(i)}, x^{(j)}) \\approx f(x^{(i)}) \\, f(x^{(j)})$$\n",
    "\n",
    "It's important to note that $s_K = \\lVert w \\rVert$, making it a measure of the model's complexity. Higher values of $s_K$ suggest that the kernel machine $h$ becomes a more complex function, which can lead to overfitting and poor generalization performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f755cf-83e2-4459-95cc-07a1050d84ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model complexity of the training projected kernel on qubit 1: 0.807\n",
      "Model complexity of the training projected kernel on qubit 4: 0.648\n",
      "Model complexity of the training fidelity kernel: 8.060\n",
      "Model complexity of the training linear kernel: 2.031\n",
      "Model complexity of the training rbf kernel: 8.010\n"
     ]
    }
   ],
   "source": [
    "from quask.evaluator import EssModelComplexityEvaluator\n",
    "\n",
    "sm_eval = EssModelComplexityEvaluator()\n",
    "sm_1 = sm_eval.evaluate(kernel=kernel_proj_1, K=K_train_1, X=None, y=np.asarray(y_train))\n",
    "print(f\"Model complexity of the training projected kernel on qubit 1: {sm_1:.3f}\")\n",
    "sm_4 = sm_eval.evaluate(kernel=kernel_proj_4, K=K_train_4, X=None, y=np.asarray(y_train))\n",
    "print(f\"Model complexity of the training projected kernel on qubit 4: {sm_4:.3f}\")\n",
    "sm_fid = sm_eval.evaluate(kernel=kernel_fid, K=K_train_fid, X=None, y=np.asarray(y_train))\n",
    "print(f\"Model complexity of the training fidelity kernel: {sm_fid:.3f}\")\n",
    "sm_lin = sm_eval.evaluate(kernel=None, K=K_train_lin, X=None, y=np.asarray(y_train))\n",
    "print(f\"Model complexity of the training linear kernel: {sm_lin:.3f}\")\n",
    "sm_rbf = sm_eval.evaluate(kernel=None, K=K_train_rbf, X=None, y=np.asarray(y_train))\n",
    "print(f\"Model complexity of the training rbf kernel: {sm_rbf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255dd89",
   "metadata": {},
   "source": [
    "We can also check if the feature maps we choose are expressive in terms of the `HaarEvaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed11b716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost (norm) of the training projected kernel on qubit 1: 0.438\n",
      "Cost (norm) of the training projected kernel on qubit 4: 0.437\n",
      "Cost (norm) of the training fidelity kernel: 0.435\n"
     ]
    }
   ],
   "source": [
    "from quask.evaluator import HaarEvaluator\n",
    "\n",
    "he = HaarEvaluator(n_bins=40, n_samples=10000)\n",
    "cost = he.evaluate(kernel=kernel_proj_1, K=K_train_1, X=None, y=None)\n",
    "print(f\"Cost (norm) of the training projected kernel on qubit 1: {cost:.3f}\")\n",
    "cost = he.evaluate(kernel=kernel_proj_4, K=K_train_4, X=None, y=None)\n",
    "print(f\"Cost (norm) of the training projected kernel on qubit 4: {cost:.3f}\")\n",
    "cost = he.evaluate(kernel=kernel_fid, K=K_train_fid, X=None, y=None)\n",
    "print(f\"Cost (norm) of the training fidelity kernel: {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e9220e-0540-4c09-a431-03382448283a",
   "metadata": {},
   "source": [
    "### Geometric difference\n",
    "\n",
    "While the quantity $s_K$ compare a kernel and the target function, the geometric difference quantifies the divergence between two kernels. \n",
    "\n",
    "Assume for the two kernel matrices $K_1, K_2$ that their trace is equal to $m$. This is a valid assumption for quantum kernels, as the inner product between unitary vectors (or corresponding density matrices) is one, which then has to be multiplied for the $m$ elements. For classical kernels, the Gram matrix needs to be normalized. \n",
    "\n",
    "The geometric difference is defined by\n",
    "$$g(K_1, K_2) = \\sqrt{\\lVert \\sqrt{K_2} K_1^{-1} \\sqrt{K_2} \\rVert_{\\infty}},$$\n",
    "where $\\lVert \\cdot \\rVert_\\infty$ is the spectral norm, i.e. the largest singular value. \n",
    "\n",
    "One should use the geometric difference to compare the quantum kernel $K_Q$ with several classical kernels $K_{C_1}, K_{C_2}, ...$. Then, $\\min g(K_C, K_Q)$ has to be calculated: \n",
    "* if this difference is small, $g(K_C, K_Q) \\ll \\sqrt{m}$, then one of the classical kernels, the one with the smallest geometric difference, is guaranteed to provide similar performances;\n",
    "* if the difference is high, $g(K_C, K_Q) \\approx \\sqrt{m}$, the quantum kernel might outperform all the classical kernels tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7e570-460c-47c0-8519-dc44698c4f54",
   "metadata": {},
   "source": [
    "### Geometry Test\n",
    "\n",
    "The geometry test, introduced by [Hua21], serves as a means to assess whether a particular dataset holds the potential for a quantum advantage or if such an advantage is unlikely. The test operates as follows:\n",
    "\n",
    "- When $g(K_C, K_Q) \\ll \\sqrt{m}$, a classical kernel exhibits behavior similar to the quantum kernel, rendering the use of the quantum kernel redundant.\n",
    "\n",
    "- When $g(K_C, K_Q) \\approx \\sqrt{m}$, the quantum kernel significantly deviates from all tested classical kernels. The outcome depends on the complexity of classical kernel machines:\n",
    "    - If the complexity of any classical kernel machine is low ($s_{K_C} \\ll m$), classical kernels perform well, and the quantum kernel's divergence from classical  $K_C$, doesn't yield superior performance.\n",
    "    - When the complexity of all classical kernel machines is high ($s_{K_C} \\approx m$), classical models struggle to learn the function $f$. In this scenario:\n",
    "        - If the quantum model's complexity is low ($s_{K_Q} \\ll m$), the quantum kernel successfully solves the task while the classical models do not.\n",
    "        - If the quantum model's complexity is high ($s_{K_Q} \\approx m$), even the quantum model struggles to solve the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "012da86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geometric difference of training projected kernel on qubit 1 with linear and rbf: [np.float64(0.869), np.float64(2.517)] - sqrt of m: 3.162\n",
      "geometric difference of training projected kernel on qubit 4 with linear and rbf: [np.float64(4.928), np.float64(2.375)] - sqrt of m: 3.162\n",
      "geometric difference of training fidelity kernel with linear and rbf: [np.float64(2.862), np.float64(1.481)] - sqrt of m: 3.162\n"
     ]
    }
   ],
   "source": [
    "from quask.evaluator import GeometricDifferenceEvaluator\n",
    "\n",
    "lam = 0.0001 # regularization \n",
    "g_eval = GeometricDifferenceEvaluator([K_train_lin,K_train_rbf], lam)\n",
    "g_p1 = g_eval.evaluate(kernel=kernel_proj_1, K=K_train_1, X=None, y=None)\n",
    "g_p1 = [round(g,3) for g in g_p1]\n",
    "g_p4 = g_eval.evaluate(kernel=kernel_proj_4, K=K_train_4, X=None, y=None)\n",
    "g_p4 = [round(g,3) for g in g_p4]\n",
    "g_fid = g_eval.evaluate(kernel=kernel_fid, K=K_train_fid, X=None, y=None)\n",
    "g_fid = [round(g,3) for g in g_fid]\n",
    "print(f\"geometric difference of training projected kernel on qubit 1 with linear and rbf: {g_p1} - sqrt of m: {np.sqrt(len(X_train)):.3f}\")\n",
    "print(f\"geometric difference of training projected kernel on qubit 4 with linear and rbf: {g_p4} - sqrt of m: {np.sqrt(len(X_train)):.3f}\")\n",
    "print(f\"geometric difference of training fidelity kernel with linear and rbf: {g_fid} - sqrt of m: {np.sqrt(len(X_train)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe0999",
   "metadata": {},
   "source": [
    "Let us follow the geometry test above:\n",
    "* for the Quantum Kernel Projected on the first qubit we have $g_{1,lin} = 0.869 < \\sqrt{m}$ and $g_{1,rbf} = 2.517 \\approx \\sqrt{m}$\n",
    "    - Then, we check the _model complexity_ of the rbf kernel and we see that is $s_{K_{rbf}} = 8.010 \\approx m$ there is chance of advantage\n",
    "    - The model complexity of the projected kernel on the first qubit is $s_{K_{p1}} = 0.807 \\ll m$\n",
    "    - We confirm there is **potential quantum advantage** over the rbf kernel for the Quantum Kernel Projected on the first qubit\n",
    "* for the Quantum Kernel Projected on the lastt qubit we have $g_{4,lin} = 4.219 \\approx \\sqrt{m}$ and $g_{4,rbf} = 1.821 \\approx \\sqrt{m}$\n",
    "    - Then, we check the _model complexity_ of the linear kernel and we see that is $s_{K_{lin}} = 2.031 < m$ so the classical kernel is still able to perform well\n",
    "    - In addition, the model complexity of the projected kernel on the last qubit is $s_{K_{p4}} = 0.648 \\ll m$ that could also indicate a poorly expressive feature map (underfitting)\n",
    "    - We confirm there is **NO quantum advantage** over the linear nor the rbf kernel for the Quantum Kernel Projected on the last qubit\n",
    "* for the Quantum Fidelity Kernel we have $g_{fid,lin} = 2.862 \\approx \\sqrt{m}$ and $g_{fid,rbf} = 1.481 \\approx \\sqrt{m}$\n",
    "    - Then, we check the _model complexity_ of the linear kernel and we see that is $s_{K_{lin}} = 2.031 < m$ so the classical kernel is still able to perform well\n",
    "    - In addition, the model complexity of the projected kernel on the last qubit is $s_{K_{fid}} = 8.060 \\ll m$ that could also indicate a too expressive feature map (overfitting)\n",
    "    - We confirm there is **NO quantum advantage** over the linear nor the rbf kernel for the Quantum Kernel Projected on the last qubit\n",
    "\n",
    "All these results are in line with the accuracies we registered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9e979-3e4e-48bb-a646-f050f6d8e953",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[Hua21\\] Huang, HY., Broughton, M., Mohseni, M. et al. Power of data in quantum machine learning. Nat Commun 12, 2631 (2021). https://doi.org/10.1038/s41467-021-22539-9"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40deb309-9858-4c34-80eb-4aa31c91c478",
   "metadata": {},
   "source": [
    ".. note::\n",
    "\n",
    "   Author's note."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
